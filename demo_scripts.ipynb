{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Tweet fetching using Twint"
      ],
      "metadata": {
        "id": "-RqcaqqN37AK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCP9Y7Cn3Y3Q"
      },
      "outputs": [],
      "source": [
        "import twint\n",
        "import pandas as pd\n",
        "import asyncio\n",
        "\n",
        "QUERIES = [\n",
        "    \"...\"\n",
        "]\n",
        "START_DATE = \"yyyy-mm-dd\"\n",
        "END_DATE = \"yyyy-mm-dd\"\n",
        "PATH = ''\n",
        "\n",
        "cities = pd.read_csv(f'{PATH}')\n",
        "cities = cities[['City', 'State']]\n",
        "\n",
        "for index, row in cities.iterrows():\n",
        "    city = row.City\n",
        "    state = row.State\n",
        "    print(f\"--------This is index {index} ---------\")\n",
        "    for query in QUERIES:        \n",
        "        c = twint.Config()\n",
        "        c.Since = START_DATE\n",
        "        c.Until = END_DATE\n",
        "        c.Location = False\n",
        "        c.User_full = False\n",
        "        c.Lang = '...'\n",
        "        # c.Profile_full = True\n",
        "        c.Search = query\n",
        "        c.Near = city\n",
        "        c.Hide_output = True\n",
        "        c.Count = False\n",
        "        c.Store_csv = True\n",
        "        # Output will be appended\n",
        "        c.Output = f\"{PATH}{state}_tweets.csv\"\n",
        "        twint.run.Search(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analyze with VADER"
      ],
      "metadata": {
        "id": "lIwt7KSC4-Ts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install vaderSentiment"
      ],
      "metadata": {
        "id": "2IsVP-sB5BvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "-4230IkE5IjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ],
      "metadata": {
        "id": "nbDWukNR5K2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def vader_preprocess(text):\n",
        "  # Remove Url\n",
        "  text = re.sub(r\"http\\S+\", \"\", text)\n",
        "  # Remove mentions\n",
        "  text = re.sub(r\"@\\S+\", \"\", text)\n",
        "  # Convert `&amp;` to `&`\n",
        "  text = re.sub(r\"&amp;\", \"&\", text)\n",
        "  text = re.sub(r\"\\n\", \"\", text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "ocnZ4K7f5MkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentiment"
      ],
      "metadata": {
        "id": "p-2AOwxC5TAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import tokenize\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "# Create a SentimentIntensityAnalyzer object.\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def vader_sent(text):\n",
        "  sentence_list = [sent for w in tokenize.sent_tokenize(text) for sent in w.split('\\n') if sent]\n",
        "  paragraphSentiments = 0.0\n",
        "  for sentence in sentence_list:\n",
        "      vs = analyzer.polarity_scores(sentence)\n",
        "      paragraphSentiments += vs[\"compound\"]\n",
        "  \n",
        "  # If list is empty\n",
        "  if len(sentence_list) == 0: \n",
        "    return 0.0, \"Neutral\"\n",
        "  else:\n",
        "    # Average sentiment (compound score)\n",
        "    avg_sent = round(paragraphSentiments / len(sentence_list), 4)\n",
        "    avg_sent = paragraphSentiments\n",
        "    if avg_sent >= 0.35 :\n",
        "        overall_sentiment = \"Positive\"\n",
        "    elif avg_sent <= - 0.05 :\n",
        "        overall_sentiment = \"Negative\"\n",
        "    else:\n",
        "        overall_sentiment = \"Neutral\"\n",
        "\n",
        "    return overall_sentiment"
      ],
      "metadata": {
        "id": "gvs3UzU05UZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read data and analyze"
      ],
      "metadata": {
        "id": "5R-Fu_mJ5uAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "\n",
        "NEGLIGBLE_MIN = 2000\n",
        "\n",
        "for filename in os.listdir(f\"{DIRECTORY}/tweets\"):\n",
        "  f = os.path.join(DIRECTORY, filename)\n",
        "  if f.endswith('.csv'):\n",
        "    region = filename[:-11]\n",
        "    print(region)\n",
        "    df = pd.read_csv(f\"{DIRECTORY}/tweets/{region}_tweets.csv\")\n",
        "    # Remove duplicates\n",
        "    df = df.drop_duplicates(['tweet'])\n",
        "    # Neglect negligible regions\n",
        "    if df.shape[0] < NEGLIGBLE_MIN: continue\n",
        "    # Extract features\n",
        "    df = df[['id','tweet','created_at']]\n",
        "    # Preprocessing\n",
        "    df['vader_preprocessed'] = df['tweet'].apply(vader_preprocess)\n",
        "    # df['vader_preprocessed'] =  t5_preprocess(df, 'tweet').apply(lambda x: \" \".join(x))\n",
        "    # Extract sentiment with vader\n",
        "    df['emotion'] = df['vader_preprocessed'].apply(vader_sent)\n",
        "\n",
        "    # change `created_at` to datetime\n",
        "    def temp(x):\n",
        "      # Trim to get only dates\n",
        "      x.created_at = x.created_at[:10]\n",
        "      return x\n",
        "    # Turn string dtype to datetime\n",
        "    df.created_at = pd.to_datetime(df.apply(temp, axis=1).created_at)\n",
        "    # Save into csv\n",
        "    df[['created_at', 'emotion']].to_csv(f'{DIRECTORY}/sentiments/{region}_tweet_sentiment.csv', index=False)"
      ],
      "metadata": {
        "id": "tDPECkSA5wnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ad-fuller and Granger Test"
      ],
      "metadata": {
        "id": "Bsb9sR5G4VNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scipy\n",
        "!pip install statsmodels\n",
        "\n",
        "from scipy.stats import kendalltau\n",
        "from statsmodels.tsa.stattools import grangercausalitytests, adfuller\n",
        "\n",
        "def fuller(data_list):\n",
        "  result = adfuller(data_list)\n",
        "  print('ADF Statistic: %f' % result[0])\n",
        "  print('p-value: %f' % result[1])\n",
        "  print('Critical Values:')\n",
        "  for key, value in result[4].items():\n",
        "    print('\\t%s: %.3f' % (key, value))\n",
        "\n",
        "# Change n to calculate high difference if adfuller tests fail for any series\n",
        "n = 0\n",
        "print(adfuller(np.diff(region_count['count'].tolist()[:], n=n)))\n",
        "print(adfuller(np.diff(beds_list_all[:], n=n)))\n",
        "\n",
        "df_granger = pd.DataFrame(data={\n",
        "    'tweets': np.diff(region_count['count'].tolist()[:], n=n),\n",
        "    'nursing': np.diff(beds_list_all[:], n=n)\n",
        "})\n",
        "grangercausalitytests(df_granger[['nursing', 'tweets']], maxlag=4)"
      ],
      "metadata": {
        "id": "yNR_yK984kBC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}